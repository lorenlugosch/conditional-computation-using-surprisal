[experiment]
seed=1
folder=experiments/80_mel

[model]
num_tokens=100
num_layers=2
num_hidden=512
num_mel_bins=80
bidirectional=True
normalize_fbank=False
tokenizer_training_text_path=librispeech-lm-norm.txt

[training]
base_path=/home/lugosch/data/LibriSpeech
lr=0.001
lr_period=1
gamma=0.9
batch_size=32
num_epochs=100

[inference]
beam_width=10
